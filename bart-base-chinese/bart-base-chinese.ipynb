{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7cfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：尽量展示所有环节和过程中出现过的所有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6411e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.224999999999994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numbers=[73,60,62.1,37.8]\n",
    "print(np.mean(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483cc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdab6e3288b41a29873269649f75ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\86158\\.cache\\huggingface\\hub\\models--fnlp--bart-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52a5117da804cd6a86d7dc0b9c2aaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07da0fcc02e64c8fbd56fce6d1787424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b9d06b151549ee85633bd57ef9ac1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf698d3313b4c5baf6a22821a5e07de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/561M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '北 京 是 中 国 的 首 都'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用bart-base-chinese预训练模型，LCSTS数据集，加以微调\n",
    "# 初步版本如下：\n",
    "# 用法：\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)  \n",
    "text2text_generator(\"北京是[MASK]的首都\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb9e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今 天 的 天 气 非 常 好 ， 我 们 去 公 园 散 步 吧\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和分词器\n",
    "model_name = \"fnlp/bart-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 示例：将输入文本进行编码\n",
    "input_text = \"今天的天气非常好，我们去公园散步吧。\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 生成文本\n",
    "output_ids = model.generate(input_ids)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 削减训练数据集，减少运行时间\n",
    "import json\n",
    "input_filename=\"E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data1.txt\" # 原文件\n",
    "output_filename=\"E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data1_cutted.txt\" # 截取新文件\n",
    "\n",
    "lines_to_extract=300000\n",
    "\n",
    "with open(input_filename,'rt',encoding='utf-8') as in_file,open(output_filename,'wt',encoding='utf-8') as out_file:\n",
    "    line_count=0\n",
    "    for line in in_file:\n",
    "        if line_count>=lines_to_extract:\n",
    "            break\n",
    "        out_file.write(line)\n",
    "        line_count+=1\n",
    "        \n",
    "print(f'Extracted {line_count} lines and saved to {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4cb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.构建数据集：LCSTS，训练集包含200万条数据集，为减少训练时间，将其截取到30万：data1_cutted.txt\n",
    "# 1.1）加载数据集\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class LCSTS(Dataset):\n",
    "    def __init__(self,data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "        \n",
    "    def load_data(self,data_file):\n",
    "        Data = {}\n",
    "        with open(data_file,'rt',encoding='utf-8') as f:\n",
    "            for idx,line in enumerate(f):\n",
    "                items = line.strip().split('!=!')\n",
    "                assert len(items) == 2\n",
    "                Data[idx] = {\n",
    "                    'title':items[0],\n",
    "                    'content':items[1],\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ad40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=LCSTS(\"E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data1_cutted.txt\")\n",
    "valid_data=LCSTS(\"E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data2.txt\")\n",
    "test_data=LCSTS(\"E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcb69e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:300000\n",
      "valid set size:10666\n",
      "test set size:1106\n",
      "{'title': '修改后的立法法全文公布', 'content': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。'}\n"
     ]
    }
   ],
   "source": [
    "# 输出数据集尺寸，并打印一个训练样本\n",
    "\n",
    "print(f'train set size:{len(train_data)}')\n",
    "print(f'valid set size:{len(valid_data)}')\n",
    "print(f'test set size:{len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0353110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1.2)分批，分词，编码\n",
    "from transformers import BertTokenizer,BartForConditionalGeneration,Text2TextGenerationPipeline\n",
    "import sentencepiece as spm\n",
    "\n",
    "model_path = \"fnlp/bart-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)# ,local_files_only=True\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path) # ,local_files_only=True\n",
    "# text2text_generator = Text2TextGenerationPipeline(model, tokenizer)  \n",
    "# text2text_generator(\"北京是[MASK]的首都\", max_length=50, do_sample=False)\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "# 分批函数\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs,batch_targets = [],[]\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['content'])\n",
    "        batch_targets.append(sample['title'])\n",
    "    # 源数据\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt')\n",
    "    # 标签数据\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")['input_ids']\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels==tokenizer.eos_token_id)[1] # [1]:取列索引\n",
    "        for idx,end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19d5f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOutOfMemoryError: CUDA out of memory.\\nTried to allocate 352.00 MiB (GPU 0; 23.64 GiB total capacity; 22.49 GiB already allocated;\\n183.75 MiB free; 23.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\\nSee documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader=DataLoader(train_data,batch_size=16,shuffle=True,collate_fn=collote_fn)\n",
    "valid_dataloader=DataLoader(valid_data,batch_size=16,shuffle=False,collate_fn=collote_fn)\n",
    "# 注：如果内存不足，batch_size改成16\n",
    "# 在租卡NVIDIA GeForce RTX 4090 D上出现的错误：\n",
    "\"\"\"\n",
    "OutOfMemoryError: CUDA out of memory.\n",
    "Tried to allocate 352.00 MiB (GPU 0; 23.64 GiB total capacity; 22.49 GiB already allocated;\n",
    "183.75 MiB free; 23.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\n",
    "See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7abdea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([64, 121]), 'token_type_ids': torch.Size([64, 121]), 'attention_mask': torch.Size([64, 121]), 'decoder_input_ids': torch.Size([64, 28]), 'labels': torch.Size([64, 28])}\n",
      "{'input_ids': tensor([[  101, 10762, 18458,  ...,     0,     0,     0],\n",
      "        [  101,  5344, 17348,  ...,     0,     0,     0],\n",
      "        [  101,  8613, 19914,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  5344,  7178,  ...,     0,     0,     0],\n",
      "        [  101, 10317,  5344,  ...,     0,     0,     0],\n",
      "        [  101,   124, 11225,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_input_ids': tensor([[  102,   101, 21992,  ...,     0,     0,     0],\n",
      "        [  102,   101,  5344,  ...,     0,     0,     0],\n",
      "        [  102,   101,  8318,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  102,   101, 44468,  ..., 15134,  6544,  5121],\n",
      "        [  102,   101,  4941,  ...,     0,     0,     0],\n",
      "        [  102,   101,  5039,  ...,     0,     0,     0]]), 'labels': tensor([[  101, 21992, 14779,  ...,     0,     0,     0],\n",
      "        [  101,  5344, 17348,  ...,     0,     0,     0],\n",
      "        [  101,  8318,  9380,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 44468,  9051,  ...,  6544,  5121,   102],\n",
      "        [  101,  4941,  7178,  ...,     0,     0,     0],\n",
      "        [  101,  5039,  6251,  ...,     0,     0,     0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 打印一个批次数据\n",
    "\n",
    "batch=next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:',{k:v.shape for k,v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c910f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d652f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.模型训练\n",
    "# 3.1）训练函数\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def train_loop(dataloader,model,optimizer,lr_scheduler,epoch,total_loss):\n",
    "    model.train()\n",
    "    model=model.to(device)\n",
    "    total=0\n",
    "    \n",
    "    progress_bar=tqdm(enumerate(dataloader),total=len(dataloader))\n",
    "    for step,batch_data in progress_bar:\n",
    "        batch_data=batch_data.to(device)\n",
    "        # 注意：BertTokenizer不支持token_type_ids，因此这里将具体元素添加进去，而不使用**\n",
    "        # 而且，有了labels,loss损失才能反向计算\n",
    "        outputs=model(input_ids=batch_data['input_ids'], labels=batch_data['labels'])\n",
    "        loss=outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        avg_loss=total_loss/(step+1)\n",
    "        progress_bar.set_description(f'loss:{avg_loss:>7f}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44313687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) 测试函数：在验证环节中添加评价指标，通常是准确率，召回率，f1,在此任务中使用rouge，包含以上指标\n",
    "# 注：解码的原因：rouge评价体系所需序列源是文本，而非数字编码\n",
    "from rouge import Rouge\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "rouge=Rouge()\n",
    "# 0-30：低分\n",
    "# 30-50：中等\n",
    "# 50-70：高分\n",
    "# 70以上：优秀\n",
    "\n",
    "def test_loop(dataloader,model,mode='Valid'):\n",
    "    assert mode in ['Valid','Test']\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    preds,labels=[],[]\n",
    "    for batch_data in dataloader:\n",
    "        batch_data=batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens=model.generate( # 1.生成预测\n",
    "                batch_data['input_ids'],\n",
    "                attention_mask=batch_data['attention_mask'],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,# 使用柱搜索\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,).cpu().numpy()\n",
    "        if isinstance(generated_tokens,tuple):\n",
    "            generated_tokens=generated_tokens[0]\n",
    "        # 2.对预测解码\n",
    "        decoded_preds=tokenizer.batch_decode(generated_tokens,skip_special_tokens=True)\n",
    "        \n",
    "        label_tokens=batch_data['labels'].cpu().numpy()\n",
    "        label_tokens=np.where(labels!=-100,label_tokens,tokenizer.pad_token_id)\n",
    "        decoded_labels=tokenizer.batch_decode(label_tokens,skip_special_tokens=True)\n",
    "        \n",
    "        # 用空格连接结果用于匹配rouge格式\n",
    "        preds+=[' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels+=[' '.join(label.strip()) for label in decoded_labels]      \n",
    "        \n",
    "    scores=rouge.get_scores(hyps=preds,refs=labels,avg=True)\n",
    "    result={key:value['f'] *100 for key,value in scores.items()}\n",
    "    result['avg']=np.mean(list(result.values()))\n",
    "    print(f\"{mode} Rouge1:{result['rouge-1']:>0.2f} Rouge2:{result['rouge-2']:>0.2f} \\\n",
    "            RougeL:{result['rouge-l']:>0.2f}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1163e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n",
      "Epoch 1/5\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74578e8f07448eeb83fe4f0aacc66de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 8.00 GiB total capacity; 6.99 GiB already allocated; 0 bytes free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     total_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     valid_rouge\u001b[38;5;241m=\u001b[39mtest_loop(valid_dataloader,model,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     rouge_avg\u001b[38;5;241m=\u001b[39mvalid_rouge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step,batch_data \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     15\u001b[0m     batch_data\u001b[38;5;241m=\u001b[39mbatch_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1728\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1724\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1725\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1726\u001b[0m         )\n\u001b[1;32m-> 1728\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1746\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1747\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1614\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1607\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1608\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1609\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1610\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1614\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1467\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1454\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1455\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1456\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1464\u001b[0m         use_cache,\n\u001b[0;32m   1465\u001b[0m     )\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1467\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:795\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    793\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    794\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states))\n\u001b[1;32m--> 795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n\u001b[0;32m    797\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mE:\\condaenv\\py3.10.9\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 8.00 GiB total capacity; 6.99 GiB already allocated; 0 bytes free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 3.3) 主循环\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "from transformers import AdamW,get_scheduler\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=1029):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "seed_everything(5)\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device} device')\n",
    "\n",
    "beam_size=4\n",
    "no_repeat_ngram_size=2\n",
    "learning_rate=2e-5\n",
    "epoch_num=5\n",
    "\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=learning_rate)\n",
    "lr_scheduler=get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader))\n",
    "\n",
    "total_loss=0.\n",
    "best_avg_rouge=0.\n",
    "for epoch in range(epoch_num):\n",
    "    print(f'Epoch {epoch+1}/{epoch_num}\\n------------------------------------')\n",
    "    total_loss=train_loop(train_dataloader,model,optimizer,lr_scheduler,epoch+1,total_loss)\n",
    "    valid_rouge=test_loop(valid_dataloader,model,mode='Valid')\n",
    "    rouge_avg=valid_rouge['avg']\n",
    "    if rouge_avg>best_avg_rouge:\n",
    "        best_avg_rouge=rouge_avg\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(),\n",
    "                  f'epoch_{epoch+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "        # 打印验证集评价指标\n",
    "        print(f'rouge_avg:{rouge_avg}')\n",
    "        \n",
    "        # 将验证集指标记录到文件\n",
    "        with open('rouge_avg.json','a') as f:\n",
    "            json.dump({'epoch':epoch+1,'rouge':rouge_avg},f)\n",
    "            f.write('\\n')# 确保在文件关闭前执行写入操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc341fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.模型测试\n",
    "test_data=LCSTS(\"data/data3.txt\")# E:\\\\NLP任务\\\\生成式任务\\\\data\\\\lcsts_tsv\\\\data3.txt\n",
    "test_dataloader=DataLoader(test_data,batch_size=32,shuffle=False,collate_fn=collote_fn)\n",
    "\n",
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load('epoch_1_valid_rouge_6.6667_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "max_input_length=512\n",
    "max_target_length=64\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources,preds,labels=[],[],[]\n",
    "    for batch_data in test_dataloader:\n",
    "        batch_data=batch_data.to(device)\n",
    "        generated_tokens=model.generate( # 1.生成预测\n",
    "            batch_data['input_ids'],\n",
    "            attention_mak=batch_data['attention_mask'],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2).cpu().numpy()\n",
    "        if isinstance(generated_tokens,tuple):\n",
    "            generated_tokens=generated_tokens[0]\n",
    "        # 2.对预测解码    \n",
    "        decoded_preds=tokenizer.batch_decode(generated_tokens,skip_special_tokens=True)\n",
    "        \n",
    "        # 转换标签并解码\n",
    "        label_tokens=batch_data['labels'].cpu().numpy()\n",
    "        label_tokens=np.where(labels!=-100,label_tokens,tokenizer.pad_token_id)\n",
    "        decoded_labels=tokenizer.batch_decode(label_tokens,skip_special_tokens=True)\n",
    "        \n",
    "        decoded_sources=tokenizer.batch_decode(\n",
    "            batch_data['input_ids'].cpu().numpy(),\n",
    "            skip_special_tokens=True,\n",
    "            use_source_tokenizer=True)\n",
    "        \n",
    "        preds+=[' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels+=[' '.join(label.strip()) for label in decoded_labels]\n",
    "        sources+=[' '.join(source.strip()) for source in decoded_sources]\n",
    "    scores=rouge.get_scores(\n",
    "        hyps=preds,refs=labels,avg=True)\n",
    "    rouges={key:value['f'] *100 for key,value in scores.items()}\n",
    "    rouges['avg']=np.mean(list(rouges.values()))\n",
    "    print(f\"Test Rouge1: {rouges['rouge-1']:>0.2f} Rouge2: {rouges['rouge-2']:>0.2f} RougeL: {rouges['rouge-l']:>0.2f}\\n\")\n",
    "    results=[]\n",
    "    for source,pred,label in zip(sources,preds,labels):\n",
    "        results.append({\n",
    "            'document':source,\n",
    "            'prediction':pred,\n",
    "            'summarization':label\n",
    "        })\n",
    "    with open('test_data_pred.json','wt',encoding='utf-8') as f:\n",
    "        for example_result in results:\n",
    "            f.write(json.dumps(example_result,ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2190922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在平台上租卡训练时如何指定GPU而非CPU\n",
    "import torch\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # 如果有多块，可以多指定\n",
    "    print(f\"There are{torch.cuda.device_count()} GPU(s) available.\")\n",
    "    print(\"Device name:\",torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    \n",
    "# 举例：可以将Tensor或模型移动到指定的设备上  \n",
    "# 例如，创建一个Tensor并将其移动到GPU上 \n",
    "tensor = torch.randn(10,10).to(device)\n",
    "\n",
    "\n",
    "# 或者，在创建模型时直接指定设备  \n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显存分配：\n",
    "# 情况1.如果你是在 Python 脚本或 Jupyter Notebook 中运行的，需要通过 os.environ 来设置环境变量。\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# 情况2.如果你是在命令行（例如终端或 Anaconda Prompt）中运行 Python 脚本，你可以在运行脚本之前设置这个环境变量：\n",
    "# bash:\n",
    "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "python your_script.py\n",
    "\n",
    "# 情况3.如果你是在 Windows 环境下运行，可以使用 set 命令：\n",
    "set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "python your_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7084cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练总结：\n",
    "# 1.训练5轮，每轮训练约25分钟\n",
    "# 2.租卡参数：NVIDIA GeForce RTX 4090 D\n",
    "# 3.缓解内存不足：看上面，削减batch_size至16，如果追求大批次，那么16是该资源前提下的下界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练后通常建议手动释放内存，不建议自动释放，手动调节更可控\n",
    "del model\n",
    "del optimizer\n",
    "del dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (ipykernel)",
   "language": "python",
   "name": "python3.10.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
